diff --git a/.DS_Store b/.DS_Store
index 9a526d9..8bada70 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/A3C.py b/A3C.py
index 4ac24bf..1273991 100644
--- a/A3C.py
+++ b/A3C.py
@@ -11,7 +11,7 @@ os.environ["OMP_NUM_THREADS"] = "1"
 
 UPDATE_GLOBAL_ITER = 10
 GAMMA = 0.99
-MAX_EP = 200
+MAX_EP = 30000
 
 env = RandomObstaclesEnv()
 N_S = env.observation_space.shape
@@ -35,7 +35,7 @@ class Worker(mp.Process):
             buffer_s, buffer_a, buffer_r = [], [], []
             ep_r = 0.
             action_list = []
-            max_steps_per_ep = 200000
+            max_steps_per_ep = 200
             step_in_ep = 0
             while True:                 
                 if self.name == 'w00':
diff --git a/__pycache__/column_env.cpython-38.pyc b/__pycache__/column_env.cpython-38.pyc
index eaf3e55..4757a67 100644
Binary files a/__pycache__/column_env.cpython-38.pyc and b/__pycache__/column_env.cpython-38.pyc differ
diff --git a/__pycache__/network.cpython-38.pyc b/__pycache__/network.cpython-38.pyc
index 4c9b435..8652a5f 100644
Binary files a/__pycache__/network.cpython-38.pyc and b/__pycache__/network.cpython-38.pyc differ
diff --git a/column_env.py b/column_env.py
index 2b79cb9..6433ef4 100644
--- a/column_env.py
+++ b/column_env.py
@@ -240,26 +240,15 @@ class RandomObstaclesEnv(gym.Env):
             self.state={'position':new_position, 'map':self.map[0:3]}
         self.reward=-1.0+new_known_pixels+new_inspected_columns*10+penalty
         self.episode_reward += self.reward
-        if action == self.action_dict['stop']:  # output info when episode ends
-            terminated = True
-            info = {
-                "info": {
-                    # "r": float(self.episode_reward),
-                    # "l": self.step_count
-                }
-            }
-            self.episode_reward = 0
-            self.step_count = 0
-        else:
-            terminated = False
-            info = {}
+        terminated = True if action == self.action_dict['stop'] else False  # output info when episode ends
+        info = {}
         truncated=True if self.step_count>10000000 else False
 
         self.rendered_frames.append([self.traj_record.copy(),self.map[0].copy(),self.map[1].copy(),self.map[2].copy()])
         return self.state, self.reward, terminated, truncated, info  
         # return self.state, self.reward, terminated, info
 
-    def render(self, mode='human'):
+    def render(self, mode='human'):  # demo function
         if mode == 'human':
             display.clear_output(wait=True)
             fig, axs = plt.subplots(1, 3)   
diff --git a/network.py b/network.py
index fdfdda0..da350d6 100644
--- a/network.py
+++ b/network.py
@@ -5,6 +5,9 @@ from utils import v_wrap, set_init, push_and_pull, record, SharedAdam
 from torch.distributions.categorical import Categorical
 class Net(nn.Module):
     def __init__(self, s_dim, a_dim):
+        '''
+            directly works as agent
+        '''
         super(Net, self).__init__()
         self.s_dim = s_dim
         self.a_dim = a_dim
diff --git a/ppo_for_planning.py b/ppo_for_planning.py
index e5f5100..2a005ad 100644
--- a/ppo_for_planning.py
+++ b/ppo_for_planning.py
@@ -31,13 +31,13 @@ def parse_args():
         help="the learning rate of the optimizer")
     parser.add_argument("--seed", type=int, default=1,
         help="seed of the experiment")
-    parser.add_argument("--total-timesteps", type=int, default=25000,
+    parser.add_argument("--total-timesteps", type=int, default=250000,
         help="total timesteps of the experiments")
     parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
         help="if toggled, `torch.backends.cudnn.deterministic=False`")
     parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
         help="if toggled, cuda will be enabled by default")
-    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=False,
         help="if toggled, this experiment will be tracked with Weights and Biases")
     parser.add_argument("--wandb-project-name", type=str, default="ppo-implementation-details",
         help="the wandb's project name")
@@ -56,7 +56,7 @@ def parse_args():
     # Algorithm specific arguments
     parser.add_argument("--num-envs", type=int, default=2,
         help="the number of parallel game environments")
-    parser.add_argument("--num-steps", type=int, default=128,
+    parser.add_argument("--num-steps", type=int, default=512,
         help="the number of steps to run in each environment per policy rollout")
     parser.add_argument("--anneal-lr", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
         help="Toggle learning rate annealing for policy and value networks")
@@ -285,7 +285,8 @@ if __name__ == "__main__":
                     old_approx_kl = (-logratio).mean()
                     approx_kl = ((ratio - 1) - logratio).mean()
                     clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
-
+                print(ratio)
+                print("------------------------------------/n/n/n")
                 mb_advantages = b_advantages[mb_inds]  # Normalize advantages
                 if args.norm_adv:
                     mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
diff --git a/runs/.DS_Store b/runs/.DS_Store
index 292b22f..e278707 100644
Binary files a/runs/.DS_Store and b/runs/.DS_Store differ
